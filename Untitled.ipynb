{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over.\n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "%run raw2nv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载数据ing\n",
      "加载完毕\n",
      "x、y拆分完毕\n",
      "开始加载词典ing\n",
      "加载完毕\n",
      "开始加载词向量ing\n",
      "加载完毕\n",
      "开始分词，转化为数字ing\n",
      "分词完毕\n",
      "拆分训练集和测试集ing\n",
      "拆分完毕\n",
      "合并x和ying\n",
      "合并完毕\n",
      "x_train shape: (160, 30)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 8s 2s/step - loss: 1.7884 - accuracy: 0.4953 - val_loss: 1.7874 - val_accuracy: 0.4688\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 1s 129ms/step - loss: 1.7785 - accuracy: 0.5832 - val_loss: 1.7749 - val_accuracy: 0.4688\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7749 - accuracy: 0.4688\n",
      "Final test loss and accuracy : [1.774943470954895, 0.46875]\n",
      "INFO:tensorflow:Assets written to: model/att\\assets\n",
      "test acc: 0.46875\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "@author: Ccccandy\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers,losses\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "\n",
    "batchsz = 32 # 批量大小\n",
    "max_review_len = 30 # 每个句子词语量\n",
    "embedding_len = 300 # 词向量长度\n",
    "epochs = 2 # 训练次数\n",
    "hiddenSizes=[128,64]\n",
    "drop=0.5\n",
    "\n",
    "# 分词 \n",
    "# in：可for循环的数据集，如['a','b',……]\n",
    "# out：[[a,],[b,],……]\n",
    "def fenci(batch):    \n",
    "    fc=[]      # 建立列表，存储每条数据的分词结果\n",
    "    for i in batch:\n",
    "        i=str(i)    # 将数据转化为字符串格式\n",
    "        i=re.sub(r'//@.*?:', '',i)   # 处理转发\n",
    "        i=re.sub(r'http://(\\w|\\.)+(/\\w+)*', '',i)   #处理超链接\n",
    "        cut = jieba.cut(i)\n",
    "        cut_list = [ i for i in cut ]\n",
    "        cut=stop(cut_list)\n",
    "        fc.append(cut)  \n",
    "    return fc\n",
    "\n",
    "# 引用停词文档\n",
    "def stopwordslist():\n",
    "    stopwords = [line.strip() for line in open('data/stopword1.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "# 过滤停词文档\n",
    "def stop(each):    \n",
    "    stopword=stopwordslist()    # 获取停词文档\n",
    "    new=[]              # 建立列表，存储过滤后的分词结果\n",
    "    for i in each:\n",
    "        if i not in stopword:\n",
    "            new.append(i)\n",
    "    return new\n",
    "\n",
    "# in：分词列表\n",
    "# out：分词转换为的数字列表\n",
    "def w2n(each,index_dict):    \n",
    "    return ([index_dict.get(i, 0) for i in each]) \n",
    "\n",
    "# 获取词向量层的权重\n",
    "def get_data(index_dict,word_vectors,num_words):\n",
    "    embedding_weights = np.zeros((num_words, 300))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]        \n",
    "    return embedding_weights\n",
    "\n",
    "# 加载数据 \n",
    "def load():\n",
    "    # 加载数据\n",
    "    print('开始加载数据ing')\n",
    "    train = pd.read_excel('data/virus_train.xlsx', usecols = [1,2],nrows=200,encoding='utf-8')  # 读取excel文件\n",
    "    print('加载完毕')    \n",
    "    a=train.values.tolist()  # 转换为列表形式    \n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in a:\n",
    "        x.append(i[0])\n",
    "        if i[1]=='happy':\n",
    "            y.append(0)\n",
    "        elif i[1]=='sad':\n",
    "            y.append(1)\n",
    "        elif i[1]=='surprise':\n",
    "            y.append(2)\n",
    "        elif i[1]=='fear':\n",
    "            y.append(3)\n",
    "        elif i[1]=='angry':\n",
    "           y.append(4)\n",
    "        elif i[1]=='neural':\n",
    "            y.append(5)\n",
    "    y=tf.one_hot(y,6)\n",
    "    print('x、y拆分完毕')\n",
    "    return x,y\n",
    "\n",
    "# 数据预处理    \n",
    "def process(x,y,index_dict):\n",
    "    data_len=len(y)\n",
    "    t_data_len=data_len//5*4\n",
    "    # 分词\n",
    "    print('开始分词，转化为数字ing')\n",
    "    fens=fenci(x)\n",
    "    xx=[w2n(i,index_dict) for i in fens]\n",
    "    print('分词完毕')    \n",
    "    # 截断和填充句子，使得等长，此处长句子保留句子后面的部分，短句子在后面填充\n",
    "    print('拆分训练集和测试集ing')\n",
    "    x_train = keras.preprocessing.sequence.pad_sequences(xx[:t_data_len], maxlen=max_review_len,padding='post')\n",
    "    x_test = keras.preprocessing.sequence.pad_sequences(xx[t_data_len:], maxlen=max_review_len,padding='post')\n",
    "    y_train = tf.constant(y[:t_data_len])\n",
    "    y_test = tf.constant(y[t_data_len:])\n",
    "    print('拆分完毕')\n",
    "    # 构建数据集，打散，批量，并丢掉最后一个不够 batchsz 的 batch\n",
    "    print('合并x和ying')\n",
    "    db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    db_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)  # 不足一个batch抛弃\n",
    "    db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    db_test = db_test.batch(batchsz, drop_remainder=True)\n",
    "    print('合并完毕')\n",
    "    # 统计数据集属性\n",
    "    print('x_train shape:', x_train.shape)    \n",
    "    return db_train,db_test\n",
    "\n",
    "# 加载词典，获取词典数量和词典矩阵\n",
    "def get_dict():    \n",
    "    # 载入词典\n",
    "    print('开始加载词典ing')\n",
    "    index_dict=np.load('other/w2n_dic.npy').item()   \n",
    "    word_vectors=np.load('other/w2v_dic.npy').item()   \n",
    "    print('加载完毕')\n",
    "    # 加载词向量矩阵\n",
    "    print('开始加载词向量ing')\n",
    "    num_words = len(word_vectors)+1\n",
    "    embedding_matrix=get_data(index_dict,word_vectors,num_words)\n",
    "    print('加载完毕')\n",
    "    return num_words,embedding_matrix,index_dict\n",
    "\n",
    "\n",
    "class RnnAttentionLayer(layers.Layer):\n",
    "    def __init__(self, attention_size, drop_rate):\n",
    "         super().__init__()\n",
    "         self.attention_size = attention_size\n",
    "         self.dropout = layers.Dropout(drop_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attention_w = self.add_weight(name = \"atten_w\", shape = (input_shape[-1], self.attention_size), initializer = tf.random_uniform_initializer(), dtype = \"float32\", trainable = True)\n",
    "        self.attention_u = self.add_weight(name = \"atten_u\", shape = (self.attention_size,), initializer = tf.random_uniform_initializer(), dtype = \"float32\", trainable = True)\n",
    "        self.attention_b = self.add_weight(name = \"atten_b\", shape = (self.attention_size,), initializer = tf.constant_initializer(0.1), dtype = \"float32\", trainable = True)    \n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = tf.tanh(tf.add(tf.tensordot(inputs, self.attention_w, axes = 1), self.attention_b))\n",
    "        x = tf.tensordot(x, self.attention_u, axes = 1)\n",
    "        x = tf.nn.softmax(x)\n",
    "        weight_out = tf.multiply(tf.expand_dims(x, -1), inputs)\n",
    "        final_out = tf.reduce_sum(weight_out, axis = 1) \n",
    "        drop_out = self.dropout(final_out, training = training)\n",
    "        return drop_out\n",
    "\n",
    "class RnnLayer(layers.Layer):\n",
    "    def __init__(self, rnn_size, drop_rate):\n",
    "        super().__init__()\n",
    "        fwd_lstm = layers.LSTM(rnn_size, return_sequences = True, go_backwards= False, dropout = drop_rate, name = \"fwd_lstm\")\n",
    "        bwd_lstm = layers.LSTM(rnn_size, return_sequences = True, go_backwards = True, dropout = drop_rate, name = \"bwd_lstm\")\n",
    "        self.bilstm = layers.Bidirectional(merge_mode = \"concat\", layer = fwd_lstm, backward_layer = bwd_lstm, name = \"bilstm\")\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        outputs = self.bilstm(inputs, training = training)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "# 构建模型\n",
    "class BiLSTMAttention(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiLSTMAttention, self).__init__()\n",
    "        \n",
    "        self.embedding = layers.Embedding(num_words, embedding_len,input_length=max_review_len,trainable=True)#不参与梯度更新\n",
    "        self.embedding.build(input_shape=(None, max_review_len))\n",
    "        self.embedding.set_weights([embedding_matrix])       \n",
    "        self.rnn_layer1 = RnnLayer(hiddenSizes[1], drop)\n",
    "        self.attention_layer = RnnAttentionLayer(hiddenSizes[-1], drop)      \n",
    "        self.f=layers.Flatten() # 打平层，方便全连接层处理\n",
    "        self.out1=layers.Dense(64, activation='relu')\n",
    "        self.out2=layers.Dense(16, activation='relu') \n",
    "        self.outlayer=layers.Dense(6, activation='softmax') \n",
    "  \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs \n",
    "        #print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        #print(x.shape)        \n",
    "        x = self.rnn_layer1(x,training = training)\n",
    "        #print(x.shape)\n",
    "        x = self.attention_layer(x,training = training)\n",
    "        #print(x.shape)\n",
    "        x = self.f(x)\n",
    "        #print(x.shape)\n",
    "        x = self.out1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.out2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.outlayer(x)\n",
    "        #print(x.shape)\n",
    "        prob = tf.sigmoid(x)\n",
    "        return prob\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    # 加载数据 \n",
    "    x,y=load()\n",
    "    num_words,embedding_matrix,index_dict=get_dict()  \n",
    "    db_train,db_test=process(x,y,index_dict) \n",
    "    \n",
    "    # 创建模型\n",
    "    model = BiLSTMAttention() \n",
    "\n",
    "    # 装配    \n",
    "    model.compile(optimizer = optimizers.Adam(0.001),\n",
    "                  loss = losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False)\n",
    "    # 训练和验证\n",
    "    model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
    "    # 测试\n",
    "    a=model.evaluate(db_test)\n",
    "    print(\"Final test loss and accuracy :\", a)\n",
    "    \n",
    "    # 保存前需要模型的compile，不然会保存失败\n",
    "    tf.saved_model.save(model, 'model/att')\n",
    "    \n",
    "    correct, total = 0,0\n",
    "    \n",
    "    for x,y in db_test: # 遍历所有测试集样本\n",
    "\n",
    "        out = model(x)\n",
    "        out = tf.argmax(out,1)\n",
    "        out=tf.cast(out,tf.int32)\n",
    "        y = tf.argmax(y, 1)\n",
    "        y=tf.cast(y,tf.int32)\n",
    "        # 统计预测正确数量\n",
    "        correct += float(tf.reduce_sum(tf.cast(tf.equal(out, y),tf.float32)))\n",
    "        # 统计预测样本总数\n",
    "        total += x.shape[0] # 计算准确率\n",
    "    print('test acc:', correct/total)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载数据ing\n",
      "加载完毕\n",
      "x、y拆分完毕\n",
      "开始加载词典ing\n",
      "加载完毕\n",
      "开始加载词向量ing\n",
      "加载完毕\n",
      "开始分词，转化为数字ing\n",
      "分词完毕\n",
      "拆分训练集和测试集ing\n",
      "拆分完毕\n",
      "合并x和ying\n",
      "合并完毕\n",
      "x_train shape: (160, 30)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 3s 664ms/step - loss: 1.6810 - accuracy: 0.3368 - val_loss: 1.6043 - val_accuracy: 0.4688\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 1.4822 - accuracy: 0.5524 - val_loss: 1.5842 - val_accuracy: 0.4688\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5842 - accuracy: 0.4688\n",
      "Final test loss and accuracy : [1.5841948986053467, 0.46875]\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x000001FA3BC427B8>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.pooling.MaxPooling1D object at 0x000001FA003B7278>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x000001FA0AAA84E0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.pooling.MaxPooling1D object at 0x000001FA0AAA8E80>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x000001FA0AAA8518>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.pooling.MaxPooling1D object at 0x000001FA54C365F8>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: model/cnn\\assets\n",
      "test acc: 0.46875\n"
     ]
    }
   ],
   "source": [
    "%run cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载数据ing\n",
      "加载完毕\n",
      "x、y拆分完毕\n",
      "开始加载词典ing\n",
      "加载完毕\n",
      "开始加载词向量ing\n",
      "加载完毕\n",
      "开始分词，转化为数字ing\n",
      "分词完毕\n",
      "拆分训练集和测试集ing\n",
      "拆分完毕\n",
      "合并x和ying\n",
      "合并完毕\n",
      "x_train shape: (160, 30)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 2s 468ms/step - loss: 1.6614 - accuracy: 0.3667 - val_loss: 1.6002 - val_accuracy: 0.4688\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 1.4761 - accuracy: 0.5438 - val_loss: 1.5843 - val_accuracy: 0.4688\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.5843 - accuracy: 0.4688\n",
      "Final test loss and accuracy : [1.5843465328216553, 0.46875]\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x000001FA0EDECE80>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.pooling.MaxPooling1D object at 0x000001FA0E79EA58>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x000001FA0E79EB70>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.pooling.MaxPooling1D object at 0x000001FA0E79EF98>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x000001FA43096EF0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.pooling.MaxPooling1D object at 0x000001FA44A54550>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: model/cnn\\assets\n",
      "test acc: 0.46875\n"
     ]
    }
   ],
   "source": [
    "%run cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载数据ing\n",
      "加载完毕\n",
      "x、y拆分完毕\n",
      "开始加载词典ing\n",
      "加载完毕\n",
      "开始加载词向量ing\n",
      "加载完毕\n",
      "开始分词，转化为数字ing\n",
      "分词完毕\n",
      "拆分训练集和测试集ing\n",
      "拆分完毕\n",
      "合并x和ying\n",
      "合并完毕\n",
      "x_train shape: (160, 30)\n",
      "Epoch 1/2\n",
      "(None, 30, 128)\n",
      "(None, 3840)\n",
      "(None, 64)\n",
      "5/5 [==============================] - 4s 793ms/step - loss: 1.6871 - accuracy: 0.4469 - val_loss: 1.6265 - val_accuracy: 0.4688\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 1.4955 - accuracy: 0.5546 - val_loss: 1.5752 - val_accuracy: 0.4688\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.5752 - accuracy: 0.4688\n",
      "Final test loss and accuracy : [1.5752098560333252, 0.46875]\n",
      "(None, 30, 128)\n",
      "(None, 3840)\n",
      "(None, 64)\n",
      "(None, 30, 128)\n",
      "(None, 3840)\n",
      "(None, 64)\n",
      "(None, 30, 128)\n",
      "(None, 3840)\n",
      "(None, 64)\n",
      "(None, 30, 128)\n",
      "(None, 3840)\n",
      "(None, 64)\n",
      "(None, 30, 128)\n",
      "(None, 3840)\n",
      "(None, 64)\n",
      "(None, 30, 128)\n",
      "(None, 3840)\n",
      "(None, 64)\n",
      "(None, 30, 128)\n",
      "(None, 3840)\n",
      "(None, 64)\n",
      "INFO:tensorflow:Assets written to: model/rnn\\assets\n",
      "(32, 30, 128)\n",
      "(32, 3840)\n",
      "(32, 64)\n",
      "test acc: 0.46875\n"
     ]
    }
   ],
   "source": [
    "%run lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始加载数据ing\n",
      "加载完毕\n",
      "x、y拆分完毕\n",
      "开始加载词典ing\n",
      "加载完毕\n",
      "开始加载词向量ing\n",
      "加载完毕\n",
      "开始分词，转化为数字ing\n",
      "分词完毕\n",
      "拆分训练集和测试集ing\n",
      "拆分完毕\n",
      "合并x和ying\n",
      "合并完毕\n",
      "x_train shape: (160, 30)\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - 5s 990ms/step - loss: 1.6083 - accuracy: 0.3120 - val_loss: 1.5885 - val_accuracy: 0.4688\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 1.4686 - accuracy: 0.6015 - val_loss: 1.5787 - val_accuracy: 0.4688\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.5787 - accuracy: 0.4688\n",
      "Final test loss and accuracy : [1.5786809921264648, 0.46875]\n",
      "INFO:tensorflow:Assets written to: model/cnn\\assets\n",
      "test acc: 0.46875\n"
     ]
    }
   ],
   "source": [
    "%run cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\anaconda\\envs\\py36\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个模型.\n",
      "loading data ...\n",
      "开始加载文件\n",
      "加载文件完毕\n",
      "x、y拆分完毕\n"
     ]
    }
   ],
   "source": [
    "# %load svm.py\n",
    "\"\"\"\n",
    "\n",
    "svm模型的构建\n",
    "\n",
    "针对usual和virus的六个情绪各生成6个模型\n",
    "放置于allmodel与illmodel文件中\n",
    "\n",
    "\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "import jieba\n",
    "import re\n",
    "\n",
    " \n",
    "n_dim = 300 # 词向量长度\n",
    " \n",
    "# 分词 \n",
    "# in：可for循环的数据集，如['a','b',……]\n",
    "# out：[[a,],[b,],……]\n",
    "def fenci(batch):    \n",
    "    fc=[]      # 建立列表，存储每条数据的分词结果\n",
    "    for i in batch:\n",
    "        i=str(i)    # 将数据转化为字符串格式\n",
    "        i=re.sub(r'//@.*?:', '',i)   # 处理转发\n",
    "        i=re.sub(r'http://(\\w|\\.)+(/\\w+)*', '',i)   #处理超链接\n",
    "        cut = jieba.cut(i)\n",
    "        cut_list = [ i for i in cut ]\n",
    "        cut=stop(cut_list)\n",
    "        fc.append(cut)  \n",
    "    return fc\n",
    "\n",
    "# 引用停词文档\n",
    "def stopwordslist():\n",
    "    stopwords = [line.strip() for line in open('data/stopword1.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "# 过滤停词文档\n",
    "def stop(each):    \n",
    "    stopword=stopwordslist()    # 获取停词文档\n",
    "    new=[]              # 建立列表，存储过滤后的分词结果\n",
    "    for i in each:\n",
    "        if i not in stopword:\n",
    "            new.append(i)\n",
    "    return new\n",
    " \n",
    " \n",
    "# 加载数据，x与y的转化（需要6次），分词进行保存\n",
    "def loadfile(i):\n",
    "    print('开始加载文件')\n",
    "    train = pd.read_excel('data/virus_train.xlsx', usecols = [1,2], encoding='utf-8')  # 读取excel文件\n",
    "    print('加载文件完毕')   \n",
    "    a=train.values.tolist()   # 转换为列表形式\n",
    "    x=[]\n",
    "    y=[]\n",
    "    if i==0:       \n",
    "        for n in a:\n",
    "            x.append(n[0])\n",
    "            if n[1]=='happy':\n",
    "                y.append(1)\n",
    "            elif n[1]=='sad':\n",
    "                y.append(0)\n",
    "            elif n[1]=='surprise':\n",
    "                y.append(0)\n",
    "            elif n[1]=='fear':\n",
    "                y.append(0)\n",
    "            elif n[1]=='angry':\n",
    "                y.append(0)\n",
    "            elif n[1]=='neural':\n",
    "                y.append(0)               \n",
    "    elif i==1:\n",
    "        for n in a:\n",
    "            x.append(n[0])\n",
    "            if n[1]=='happy':\n",
    "                y.append(0)\n",
    "            elif n[1]=='sad':\n",
    "                y.append(1)\n",
    "            elif n[1]=='surprise':\n",
    "                y.append(0)\n",
    "            elif n[1]=='fear':\n",
    "                y.append(0)\n",
    "            elif n[1]=='angry':\n",
    "                y.append(0)\n",
    "            elif n[1]=='neural':\n",
    "                y.append(0)\n",
    "    elif i==2:\n",
    "        for n in a:\n",
    "            x.append(n[0])\n",
    "            if n[1]=='happy':\n",
    "                y.append(0)\n",
    "            elif n[1]=='sad':\n",
    "                y.append(0)\n",
    "            elif n[1]=='surprise':\n",
    "                y.append(1)\n",
    "            elif n[1]=='fear':\n",
    "                y.append(0)\n",
    "            elif n[1]=='angry':\n",
    "                y.append(0)\n",
    "            elif n[1]=='neural':\n",
    "                y.append(0)\n",
    "    elif i==3:\n",
    "        for n in a:\n",
    "            x.append(n[0])\n",
    "            if n[1]=='happy':\n",
    "                y.append(0)\n",
    "            elif n[1]=='sad':\n",
    "                y.append(0)\n",
    "            elif n[1]=='surprise':\n",
    "                y.append(0)\n",
    "            elif n[1]=='fear':\n",
    "                y.append(1)\n",
    "            elif n[1]=='angry':\n",
    "                y.append(0)\n",
    "            elif n[1]=='neural':\n",
    "                y.append(0)\n",
    "                \n",
    "    elif i==4:\n",
    "        for n in a:\n",
    "            x.append(n[0])\n",
    "            if n[1]=='happy':\n",
    "                y.append(0)\n",
    "            elif n[1]=='sad':\n",
    "                y.append(0)\n",
    "            elif n[1]=='surprise':\n",
    "                y.append(0)\n",
    "            elif n[1]=='fear':\n",
    "                y.append(0)\n",
    "            elif n[1]=='angry':\n",
    "                y.append(1)\n",
    "            elif n[1]=='neural':\n",
    "                y.append(0)               \n",
    "    elif i==5:\n",
    "        for n in a:\n",
    "            x.append(n[0])\n",
    "            if n[1]=='happy':\n",
    "                y.append(0)\n",
    "            elif n[1]=='sad':\n",
    "                y.append(0)\n",
    "            elif n[1]=='surprise':\n",
    "                y.append(0)\n",
    "            elif n[1]=='fear':\n",
    "                y.append(0)\n",
    "            elif n[1]=='angry':\n",
    "                y.append(0)\n",
    "            elif n[1]=='neural':\n",
    "                y.append(1)    \n",
    "    print('x、y拆分完毕')        \n",
    "    x=fenci(x)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)    \n",
    "    np.save('data/y_train.npy',y_train)\n",
    "    np.save('data/y_test.npy',y_test)\n",
    "    return x, x_train, x_test, y_train, y_test\n",
    "  \n",
    "# 获取单个句子中词语的词向量，并获取平均后的词向量\n",
    "def buildWordVector(text, size, imdb_w2v):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape((1, size))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count \n",
    "    return vec\n",
    " \n",
    " \n",
    "# 获取word2vec的词向量词典，并将x中的数据加入其中进行训练，将训练与验证数据转化为词向量\n",
    "def get_train_vecs(x, x_train, x_test):\n",
    "    # Initialize model and build vocab\n",
    "    imdb_w2v = Word2Vec.load('word2vec.bin')\n",
    "    imdb_w2v.build_vocab(x, update=True)\n",
    "    imdb_w2v.train(x, total_examples=imdb_w2v.corpus_count, epochs=20)\n",
    "    imdb_w2v.save(\"word2vec.bin\") \n",
    "    \n",
    "    train_vecs = np.concatenate([buildWordVector(z, n_dim, imdb_w2v) for z in x_train]) \n",
    "    np.save('data/train_vecs.npy', train_vecs)\n",
    "    print(train_vecs.shape)\n",
    "\n",
    "    test_vecs = np.concatenate([buildWordVector(z, n_dim, imdb_w2v) for z in x_test])\n",
    "    np.save('data/test_vecs.npy', test_vecs)\n",
    "    print(test_vecs.shape)\n",
    "    \n",
    "    return train_vecs, test_vecs\n",
    " \n",
    " \n",
    "# 训练SVC模型\n",
    "def svm_train(train_vecs, y_train, test_vecs, y_test,i):\n",
    "    clf = SVC(kernel='rbf', verbose=True, probability=True)\n",
    "    clf.fit(train_vecs, y_train)\n",
    "    joblib.dump(clf, 'model/svm/svm_model'+str(i)+'.pkl')\n",
    "    print(clf.score(test_vecs, y_test))\n",
    " \n",
    " \n",
    "# 对SVC模型进行预测\n",
    "def svm_predict(test,i):\n",
    "    clf = joblib.load('model/svm/svm_model'+str(i)+'.pkl')\n",
    "    imdb_w2v = Word2Vec.load('word2vec.bin')\n",
    "    \n",
    "    str_sege = fenci(test)\n",
    "    str_vecs = [buildWordVector(z, n_dim, imdb_w2v) for z in str_sege]\n",
    "    pred_result = [clf.predict(i) for i in str_vecs]\n",
    "    print(pred_result)\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    for i in range(6):\n",
    "        n=i+1\n",
    "        print('第'+str(n)+'个模型.')           \n",
    "        print(\"loading data ...\")\n",
    "        x, x_train, x_test, y_train, y_test = loadfile(i)\n",
    "        print(\"train word2vec model and get the input of svm model\")\n",
    "        train_vecs, test_vecs = get_train_vecs(x, x_train, x_test)\n",
    "        print(\"train svm model...\")        \n",
    "        svm_train(train_vecs, y_train, test_vecs, y_test,i)\n",
    "        print('第'+str(n)+'个模型完成.')      \n",
    "    print(\"use svm model to predict...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
